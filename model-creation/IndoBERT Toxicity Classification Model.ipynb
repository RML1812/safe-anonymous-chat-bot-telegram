{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndoBERT Toxicity Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Radit1812\\Documents\\Kuliah\\Skripsi\\safe-anonymous-chat-bot-telegram\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech, Abusive Speech, SARA, Pornography, Radicalism, Defamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate Speech</th>\n",
       "      <th>Abusive Speech</th>\n",
       "      <th>SARA</th>\n",
       "      <th>Pornography</th>\n",
       "      <th>Radicalism</th>\n",
       "      <th>Defamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, Hate Speech, Abusive Speech, SARA, Pornography, Radicalism, Defamation]\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(columns=[\"Text\", \"Hate Speech\", \"Abusive Speech\", \"SARA\", \"Pornography\", \"Radicalism\", \"Defamation\"])\n",
    "toxic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : Netifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_path = \"./data/netifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_train_path = netifier_path + \"/processed_train.csv\"\n",
    "netifier_test_path = netifier_path + \"/processed_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_train = pd.read_csv(netifier_train_path)\n",
    "netifier_test = pd.read_csv(netifier_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netifier_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.BUKAN CM SPANDUK PROF,VIDEO2 ORASI MEREKA, B...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>bukan hanya spanduk prof video orasi mereka bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@memeqbeceq gy sange'gatel yh tetek'memekY drn...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gy sange gatel yh tetek memeky drnjng tempat t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pertama kali denger lagunya enk bgt in dan png...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pertama kali denger lagunya enk sekali in dan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astajim, ini pasti yg kasih penghargaan ke ibu...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>astajim ini pasti yang kasih penghargan ke ibu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beda kalo disini kalo komplain lgs di bully am...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beda kalau di sini kalau keluhan langsung di b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  1.BUKAN CM SPANDUK PROF,VIDEO2 ORASI MEREKA, B...    twitter           0   \n",
       "1  @memeqbeceq gy sange'gatel yh tetek'memekY drn...    twitter           1   \n",
       "2  Pertama kali denger lagunya enk bgt in dan png...  instagram           0   \n",
       "3  astajim, ini pasti yg kasih penghargaan ke ibu...     kaskus           0   \n",
       "4  beda kalo disini kalo komplain lgs di bully am...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            1                     0   \n",
       "1     0            0                     0   \n",
       "2     0            0                     0   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  bukan hanya spanduk prof video orasi mereka bu...  \n",
       "1  gy sange gatel yh tetek memeky drnjng tempat t...  \n",
       "2  pertama kali denger lagunya enk sekali in dan ...  \n",
       "3  astajim ini pasti yang kasih penghargan ke ibu...  \n",
       "4  beda kalau di sini kalau keluhan langsung di b...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netifier_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6995 entries, 0 to 6994\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   original_text         6995 non-null   object\n",
      " 1   source                6995 non-null   object\n",
      " 2   pornografi            6995 non-null   int64 \n",
      " 3   sara                  6995 non-null   int64 \n",
      " 4   radikalisme           6995 non-null   int64 \n",
      " 5   pencemaran_nama_baik  6995 non-null   int64 \n",
      " 6   processed_text        6995 non-null   object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 382.7+ KB\n"
     ]
    }
   ],
   "source": [
    "netifier_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 778 entries, 0 to 777\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   original_text         778 non-null    object\n",
      " 1   source                778 non-null    object\n",
      " 2   pornografi            778 non-null    int64 \n",
      " 3   sara                  778 non-null    int64 \n",
      " 4   radikalisme           778 non-null    int64 \n",
      " 5   pencemaran_nama_baik  778 non-null    int64 \n",
      " 6   processed_text        778 non-null    object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 42.7+ KB\n"
     ]
    }
   ],
   "source": [
    "netifier_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess:**\n",
    "- Translate text-based emojis\n",
    "- Remove excessive newline\n",
    "- Remove kaskus formatting\n",
    "- Remove url\n",
    "- Remove excessive whitespace\n",
    "- Tokenize text\n",
    "- Transform slang words\n",
    "- Remove non aplhabet\n",
    "- Remove twitter & instagram formatting\n",
    "- Remove Repeating Characters\n",
    "\n",
    "Conclusion: *clean enough, recommend to use stemming and stopword*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech (subcategories: HS_Individual, HS_Group, HS_Religion, HS_Race, HS_Physical, HS_Gender, HS_Other)\n",
    "Abusive Speech (subcategories: HS_Weak, HS_Moderate, HS_Strong)\n",
    "SARA (covering HS_Religion, HS_Race, HS_Group)\n",
    "Pornography (for pornografi)\n",
    "Radicalism (for radikalisme)\n",
    "Defamation (for HS_Individual, pencemaran_nama_baik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6995 entries, 0 to 6994\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            6995 non-null   object\n",
      " 1   Hate Speech     6995 non-null   object\n",
      " 2   Abusive Speech  6995 non-null   object\n",
      " 3   SARA            6995 non-null   object\n",
      " 4   Pornography     6995 non-null   object\n",
      " 5   Radicalism      6995 non-null   object\n",
      " 6   Defamation      6995 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 382.7+ KB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': netifier_train['processed_text'],\n",
    "    'SARA': netifier_train['sara'],\n",
    "    'Pornography': netifier_train['pornografi'],\n",
    "    'Radicalism': netifier_train['radikalisme'],\n",
    "    'Defamation': netifier_train['pencemaran_nama_baik'],\n",
    "    'Hate Speech': 0,\n",
    "    'Abusive Speech': 0\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7773 entries, 0 to 7772\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            7773 non-null   object\n",
      " 1   Hate Speech     7773 non-null   object\n",
      " 2   Abusive Speech  7773 non-null   object\n",
      " 3   SARA            7773 non-null   object\n",
      " 4   Pornography     7773 non-null   object\n",
      " 5   Radicalism      7773 non-null   object\n",
      " 6   Defamation      7773 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 425.2+ KB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': netifier_test['processed_text'],\n",
    "    'SARA': netifier_test['sara'],\n",
    "    'Pornography': netifier_test['pornografi'],\n",
    "    'Radicalism': netifier_test['radikalisme'],\n",
    "    'Defamation': netifier_test['pencemaran_nama_baik'],\n",
    "    'Hate Speech': 0,\n",
    "    'Abusive Speech': 0\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : okkyibrahim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "okkyibrahim_path = \"./data/okkyibrahim/preprocessed_indonesian_toxic_tweet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cowok usaha lacak perhati gue lantas remeh per...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat tau edan sarap gue gaul cigax jifla cal ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41 kadang pikir percaya tuhan jatuh kali kali ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ku tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  cowok usaha lacak perhati gue lantas remeh per...   1        1   \n",
       "1  telat tau edan sarap gue gaul cigax jifla cal ...   0        1   \n",
       "2  41 kadang pikir percaya tuhan jatuh kali kali ...   0        0   \n",
       "3                            ku tau mata sipit lihat   0        0   \n",
       "4          kaum cebong kafir lihat dongok dungu haha   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okkyibrahim = pd.read_csv(okkyibrahim_path)\n",
    "okkyibrahim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13169 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13169 non-null  int64 \n",
      " 2   Abusive        13169 non-null  int64 \n",
      " 3   HS_Individual  13169 non-null  int64 \n",
      " 4   HS_Group       13169 non-null  int64 \n",
      " 5   HS_Religion    13169 non-null  int64 \n",
      " 6   HS_Race        13169 non-null  int64 \n",
      " 7   HS_Physical    13169 non-null  int64 \n",
      " 8   HS_Gender      13169 non-null  int64 \n",
      " 9   HS_Other       13169 non-null  int64 \n",
      " 10  HS_Weak        13169 non-null  int64 \n",
      " 11  HS_Moderate    13169 non-null  int64 \n",
      " 12  HS_Strong      13169 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values present: 48\n"
     ]
    }
   ],
   "source": [
    "count_nan = okkyibrahim['Tweet'].isnull().sum()\n",
    "print('Number of NaN values present: ' + str(count_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess:**\n",
    "- Lower casing all text,\n",
    "- Data cleaning by removing unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation\n",
    "- Normalization using 'Alay' dictionary\n",
    "- Stemming using PySastrawi\n",
    "- Stop words removal using list from\n",
    "\n",
    "Conclusion: *need to remove hexadecimal encoding, drop NaN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13121 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13121 non-null  int64 \n",
      " 2   Abusive        13121 non-null  int64 \n",
      " 3   HS_Individual  13121 non-null  int64 \n",
      " 4   HS_Group       13121 non-null  int64 \n",
      " 5   HS_Religion    13121 non-null  int64 \n",
      " 6   HS_Race        13121 non-null  int64 \n",
      " 7   HS_Physical    13121 non-null  int64 \n",
      " 8   HS_Gender      13121 non-null  int64 \n",
      " 9   HS_Other       13121 non-null  int64 \n",
      " 10  HS_Weak        13121 non-null  int64 \n",
      " 11  HS_Moderate    13121 non-null  int64 \n",
      " 12  HS_Strong      13121 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim = okkyibrahim.dropna(subset=['Tweet'])\n",
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilih kepala daerah 2018 aman anti h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gue selesai re watch aldnoah zero kampret 2 ka...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admin belanja po nak makan ais kepal milo ais ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enak ngewe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "6  deklarasi pilih kepala daerah 2018 aman anti h...   0        0   \n",
       "7  gue selesai re watch aldnoah zero kampret 2 ka...   0        1   \n",
       "8  admin belanja po nak makan ais kepal milo ais ...   0        0   \n",
       "9                                         enak ngewe   0        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "6              0         0            0        0            0          0   \n",
       "7              0         0            0        0            0          0   \n",
       "8              0         0            0        0            0          0   \n",
       "9              0         0            0        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "6         0        0            0          0  \n",
       "7         0        0            0          0  \n",
       "8         0        0            0          0  \n",
       "9         0        0            0          0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "okkyibrahim['Tweet'] = okkyibrahim['Tweet'].apply(clean_text)\n",
    "okkyibrahim.iloc[6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13121 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13121 non-null  int64 \n",
      " 2   Abusive        13121 non-null  int64 \n",
      " 3   HS_Individual  13121 non-null  int64 \n",
      " 4   HS_Group       13121 non-null  int64 \n",
      " 5   HS_Religion    13121 non-null  int64 \n",
      " 6   HS_Race        13121 non-null  int64 \n",
      " 7   HS_Physical    13121 non-null  int64 \n",
      " 8   HS_Gender      13121 non-null  int64 \n",
      " 9   HS_Other       13121 non-null  int64 \n",
      " 10  HS_Weak        13121 non-null  int64 \n",
      " 11  HS_Moderate    13121 non-null  int64 \n",
      " 12  HS_Strong      13121 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20894 entries, 0 to 20893\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            20894 non-null  object\n",
      " 1   Hate Speech     20894 non-null  object\n",
      " 2   Abusive Speech  20894 non-null  object\n",
      " 3   SARA            20894 non-null  object\n",
      " 4   Pornography     20894 non-null  object\n",
      " 5   Radicalism      20894 non-null  object\n",
      " 6   Defamation      20894 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': okkyibrahim['Tweet'],\n",
    "    'SARA': okkyibrahim[['HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']].max(axis=1),\n",
    "    'Pornography': 0,\n",
    "    'Radicalism': 0,\n",
    "    'Defamation': okkyibrahim['HS_Individual'],\n",
    "    'Hate Speech': okkyibrahim[['HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']].max(axis=1),\n",
    "    'Abusive Speech': okkyibrahim[['HS_Weak', 'HS_Moderate', 'HS_Strong']].max(axis=1)\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate Speech</th>\n",
       "      <th>Abusive Speech</th>\n",
       "      <th>SARA</th>\n",
       "      <th>Pornography</th>\n",
       "      <th>Radicalism</th>\n",
       "      <th>Defamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Hate Speech  \\\n",
       "0  jabar memang provinsi barokah boleh juga dan n...           0   \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...           0   \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...           0   \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...           0   \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...           0   \n",
       "\n",
       "  Abusive Speech SARA Pornography Radicalism Defamation  \n",
       "0              0    0           0          0          1  \n",
       "1              0    0           0          0          0  \n",
       "2              0    1           0          1          1  \n",
       "3              0    0           0          0          0  \n",
       "4              0    0           0          0          0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20894 entries, 0 to 20893\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            20894 non-null  object\n",
      " 1   Hate Speech     20894 non-null  int64 \n",
      " 2   Abusive Speech  20894 non-null  int64 \n",
      " 3   SARA            20894 non-null  int64 \n",
      " 4   Pornography     20894 non-null  int64 \n",
      " 5   Radicalism      20894 non-null  int64 \n",
      " 6   Defamation      20894 non-null  int64 \n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "category_columns = ['Hate Speech', 'Abusive Speech', 'SARA', 'Pornography', 'Radicalism', 'Defamation']\n",
    "toxic_df[category_columns] = toxic_df[category_columns].astype(int)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hate Speech       5552\n",
       "Abusive Speech    5552\n",
       "SARA              6790\n",
       "Pornography       1747\n",
       "Radicalism        1276\n",
       "Defamation        5992\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df[category_columns].apply(lambda col: (col == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 19828\n",
      "Testing set size: 13536\n"
     ]
    }
   ],
   "source": [
    "train_df_list = []\n",
    "test_df_list = []\n",
    "\n",
    "# Split the data for each category\n",
    "for category in category_columns:\n",
    "    # Select rows where the category is 1 (positive samples)\n",
    "    positive_samples = toxic_df[toxic_df[category] == 1]\n",
    "    negative_samples = toxic_df[toxic_df[category] == 0]\n",
    "    \n",
    "    # Split the positive samples into train and test sets\n",
    "    train_pos, test_pos = train_test_split(positive_samples, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split the negative samples into train and test sets\n",
    "    train_neg, test_neg = train_test_split(negative_samples, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Combine the positive and negative samples for this category\n",
    "    train_df_list.append(train_pos)\n",
    "    train_df_list.append(train_neg)\n",
    "    test_df_list.append(test_pos)\n",
    "    test_df_list.append(test_neg)\n",
    "\n",
    "# Concatenate all the individual splits to form the final train and test DataFrames\n",
    "train_df = pd.concat(train_df_list).drop_duplicates().reset_index(drop=True)\n",
    "test_df = pd.concat(test_df_list).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Radit1812\\Documents\\Kuliah\\Skripsi\\safe-anonymous-chat-bot-telegram\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Radit1812\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(df, max_len=128):\n",
    "    return tokenizer(\n",
    "        df['Text'].tolist(), \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_len, \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize_text(train_df)\n",
    "test_encodings = tokenize_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[category_columns].values\n",
    "test_labels = test_df[category_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('indolem/indobert-base-uncased', num_labels=len(category_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Radit1812\\Documents\\Kuliah\\Skripsi\\safe-anonymous-chat-bot-telegram\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3425656010787333\n",
      "Epoch 2/3, Loss: 0.2378291197782082\n",
      "Epoch 3/3, Loss: 0.19856978204521922\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.append(logits.sigmoid().cpu().numpy())\n",
    "        true_labels.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Hate Speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97      9856\n",
      "         1.0       0.89      0.96      0.92      3680\n",
      "\n",
      "    accuracy                           0.96     13536\n",
      "   macro avg       0.94      0.96      0.95     13536\n",
      "weighted avg       0.96      0.96      0.96     13536\n",
      "\n",
      "Classification report for Abusive Speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97      9856\n",
      "         1.0       0.89      0.95      0.92      3680\n",
      "\n",
      "    accuracy                           0.96     13536\n",
      "   macro avg       0.94      0.96      0.95     13536\n",
      "weighted avg       0.96      0.96      0.96     13536\n",
      "\n",
      "Classification report for SARA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.93      0.94      9055\n",
      "         1.0       0.87      0.92      0.89      4481\n",
      "\n",
      "    accuracy                           0.93     13536\n",
      "   macro avg       0.91      0.92      0.92     13536\n",
      "weighted avg       0.93      0.93      0.93     13536\n",
      "\n",
      "Classification report for Pornography:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99     12574\n",
      "         1.0       0.95      0.89      0.92       962\n",
      "\n",
      "    accuracy                           0.99     13536\n",
      "   macro avg       0.97      0.94      0.96     13536\n",
      "weighted avg       0.99      0.99      0.99     13536\n",
      "\n",
      "Classification report for Radicalism:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99     12700\n",
      "         1.0       0.77      0.93      0.84       836\n",
      "\n",
      "    accuracy                           0.98     13536\n",
      "   macro avg       0.88      0.96      0.92     13536\n",
      "weighted avg       0.98      0.98      0.98     13536\n",
      "\n",
      "Classification report for Defamation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.92      0.91      9710\n",
      "         1.0       0.78      0.77      0.77      3826\n",
      "\n",
      "    accuracy                           0.87     13536\n",
      "   macro avg       0.85      0.84      0.84     13536\n",
      "weighted avg       0.87      0.87      0.87     13536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "for i, col in enumerate(category_columns):\n",
    "    print(f\"Classification report for {col}:\")\n",
    "    print(classification_report(true_labels[:, i], binary_predictions[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categories(sentence, model, tokenizer, threshold=0.5):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "    probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "    return probabilities[0], predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate Speech: Probability = 0.9837, Prediction = Yes\n",
      "Abusive Speech: Probability = 0.9844, Prediction = Yes\n",
      "SARA: Probability = 0.9852, Prediction = Yes\n",
      "Pornography: Probability = 0.0135, Prediction = No\n",
      "Radicalism: Probability = 0.0163, Prediction = No\n",
      "Defamation: Probability = 0.9256, Prediction = Yes\n"
     ]
    }
   ],
   "source": [
    "sentence = \"jokowi goblok korupsi mulu ga sholat\"\n",
    "probabilities, predictions = predict_categories(sentence, model, tokenizer)\n",
    "\n",
    "for i, category in enumerate(category_columns):\n",
    "    print(f\"{category}: Probability = {probabilities[i]:.4f}, Prediction = {'Yes' if predictions[i] == 1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model-export\\\\tokenizer_config.json',\n",
       " './model-export\\\\special_tokens_map.json',\n",
       " './model-export\\\\vocab.txt',\n",
       " './model-export\\\\added_tokens.json')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./model-export\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
