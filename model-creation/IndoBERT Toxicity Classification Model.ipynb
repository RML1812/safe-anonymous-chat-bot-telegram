{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndoBERTweet Toxic Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.metrics import classification_report, hamming_loss, jaccard_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech, Abusive Speech, SARA, Radicalism, Defamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate Speech</th>\n",
       "      <th>Abusive Speech</th>\n",
       "      <th>SARA</th>\n",
       "      <th>Radicalism</th>\n",
       "      <th>Defamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, Hate Speech, Abusive Speech, SARA, Radicalism, Defamation]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(columns=[\"Text\", \"Hate Speech\", \"Abusive Speech\", \"SARA\", \"Radicalism\", \"Defamation\"])\n",
    "toxic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset : Netifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_path = \"./data/netifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_train_path = netifier_path + \"/processed_train.csv\"\n",
    "netifier_test_path = netifier_path + \"/processed_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "netifier_train = pd.read_csv(netifier_train_path)\n",
    "netifier_test = pd.read_csv(netifier_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>jabar memang provinsi barokah boleh juga dan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@verosvante kita2 aja nitizen yang pada kepo,t...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kita saja nitizen yang pada penasaran toh kelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#SidangAhok smg sipenista agama n ateknya mat...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sidangahok semoga sipenista agama dan ateknya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bolususulembang.jkt barusan baca undang2 ini....</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta barusan baca undang ini tetap dibedaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bikin anak mulu lu nof \\nkaga mikir apa kasian...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>buat anak melulu kamu nof nkaga mikir apa kasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  [QUOTE=jessepinkman16;5a50ac34d89b093f368b456e...     kaskus           0   \n",
       "1  @verosvante kita2 aja nitizen yang pada kepo,t...  instagram           0   \n",
       "2  \"#SidangAhok smg sipenista agama n ateknya mat...    twitter           0   \n",
       "3  @bolususulembang.jkt barusan baca undang2 ini....  instagram           0   \n",
       "4  bikin anak mulu lu nof \\nkaga mikir apa kasian...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            0                     1   \n",
       "1     0            0                     0   \n",
       "2     1            1                     1   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  jabar memang provinsi barokah boleh juga dan n...  \n",
       "1  kita saja nitizen yang pada penasaran toh kelu...  \n",
       "2  sidangahok semoga sipenista agama dan ateknya ...  \n",
       "3  jakarta barusan baca undang ini tetap dibedaka...  \n",
       "4  buat anak melulu kamu nof nkaga mikir apa kasi...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netifier_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.BUKAN CM SPANDUK PROF,VIDEO2 ORASI MEREKA, B...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>bukan hanya spanduk prof video orasi mereka bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@memeqbeceq gy sange'gatel yh tetek'memekY drn...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gy sange gatel yh tetek memeky drnjng tempat t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pertama kali denger lagunya enk bgt in dan png...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pertama kali denger lagunya enk sekali in dan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astajim, ini pasti yg kasih penghargaan ke ibu...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>astajim ini pasti yang kasih penghargan ke ibu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beda kalo disini kalo komplain lgs di bully am...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beda kalau di sini kalau keluhan langsung di b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text     source  pornografi  \\\n",
       "0  1.BUKAN CM SPANDUK PROF,VIDEO2 ORASI MEREKA, B...    twitter           0   \n",
       "1  @memeqbeceq gy sange'gatel yh tetek'memekY drn...    twitter           1   \n",
       "2  Pertama kali denger lagunya enk bgt in dan png...  instagram           0   \n",
       "3  astajim, ini pasti yg kasih penghargaan ke ibu...     kaskus           0   \n",
       "4  beda kalo disini kalo komplain lgs di bully am...     kaskus           0   \n",
       "\n",
       "   sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0     0            1                     0   \n",
       "1     0            0                     0   \n",
       "2     0            0                     0   \n",
       "3     0            0                     0   \n",
       "4     0            0                     0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  bukan hanya spanduk prof video orasi mereka bu...  \n",
       "1  gy sange gatel yh tetek memeky drnjng tempat t...  \n",
       "2  pertama kali denger lagunya enk sekali in dan ...  \n",
       "3  astajim ini pasti yang kasih penghargan ke ibu...  \n",
       "4  beda kalau di sini kalau keluhan langsung di b...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netifier_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6995 entries, 0 to 6994\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   original_text         6995 non-null   object\n",
      " 1   source                6995 non-null   object\n",
      " 2   pornografi            6995 non-null   int64 \n",
      " 3   sara                  6995 non-null   int64 \n",
      " 4   radikalisme           6995 non-null   int64 \n",
      " 5   pencemaran_nama_baik  6995 non-null   int64 \n",
      " 6   processed_text        6995 non-null   object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 382.7+ KB\n"
     ]
    }
   ],
   "source": [
    "netifier_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 778 entries, 0 to 777\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   original_text         778 non-null    object\n",
      " 1   source                778 non-null    object\n",
      " 2   pornografi            778 non-null    int64 \n",
      " 3   sara                  778 non-null    int64 \n",
      " 4   radikalisme           778 non-null    int64 \n",
      " 5   pencemaran_nama_baik  778 non-null    int64 \n",
      " 6   processed_text        778 non-null    object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 42.7+ KB\n"
     ]
    }
   ],
   "source": [
    "netifier_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessed:**\n",
    "- Translate text-based emojis\n",
    "- Remove excessive newline\n",
    "- Remove kaskus formatting\n",
    "- Remove url\n",
    "- Remove excessive whitespace\n",
    "- Tokenize text\n",
    "- Transform slang words\n",
    "- Remove non aplhabet\n",
    "- Remove twitter & instagram formatting\n",
    "- Remove Repeating Characters\n",
    "\n",
    "Conclusion: *clean enough, recommend to use stemming and stopword*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech (subcategories: HS_Individual, HS_Group, HS_Religion, HS_Race, HS_Physical, HS_Gender, HS_Other)\n",
    "Abusive Speech (subcategories: HS_Weak, HS_Moderate, HS_Strong)\n",
    "SARA (covering sara, HS_Religion, HS_Race, HS_Group)\n",
    "Pornography (for pornografi)\n",
    "Radicalism (for radikalisme)\n",
    "Defamation (for HS_Individual, pencemaran_nama_baik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6995 entries, 0 to 6994\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            6995 non-null   object\n",
      " 1   Hate Speech     6995 non-null   object\n",
      " 2   Abusive Speech  6995 non-null   object\n",
      " 3   SARA            6995 non-null   object\n",
      " 4   Radicalism      6995 non-null   object\n",
      " 5   Defamation      6995 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 328.0+ KB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': netifier_train['processed_text'],\n",
    "    'SARA': netifier_train['sara'],\n",
    "    'Radicalism': netifier_train['radikalisme'],\n",
    "    'Defamation': netifier_train['pencemaran_nama_baik'],\n",
    "    'Hate Speech': netifier_train[['sara', 'radikalisme', 'pencemaran_nama_baik']].max(axis=1),\n",
    "    'Abusive Speech': netifier_train['pornografi']\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7773 entries, 0 to 7772\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            7773 non-null   object\n",
      " 1   Hate Speech     7773 non-null   object\n",
      " 2   Abusive Speech  7773 non-null   object\n",
      " 3   SARA            7773 non-null   object\n",
      " 4   Radicalism      7773 non-null   object\n",
      " 5   Defamation      7773 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 364.5+ KB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': netifier_test['processed_text'],\n",
    "    'SARA': netifier_test['sara'],\n",
    "    'Radicalism': netifier_test['radikalisme'],\n",
    "    'Defamation': netifier_test['pencemaran_nama_baik'],\n",
    "    'Hate Speech': netifier_test[['sara', 'radikalisme', 'pencemaran_nama_baik']].max(axis=1),\n",
    "    'Abusive Speech': netifier_test['pornografi']\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset : okkyibrahim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "okkyibrahim_path = \"./data/okkyibrahim/preprocessed_indonesian_toxic_tweet.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cowok usaha lacak perhati gue lantas remeh per...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat tau edan sarap gue gaul cigax jifla cal ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41 kadang pikir percaya tuhan jatuh kali kali ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ku tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  cowok usaha lacak perhati gue lantas remeh per...   1        1   \n",
       "1  telat tau edan sarap gue gaul cigax jifla cal ...   0        1   \n",
       "2  41 kadang pikir percaya tuhan jatuh kali kali ...   0        0   \n",
       "3                            ku tau mata sipit lihat   0        0   \n",
       "4          kaum cebong kafir lihat dongok dungu haha   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okkyibrahim = pd.read_csv(okkyibrahim_path)\n",
    "okkyibrahim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13169 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13169 non-null  int64 \n",
      " 2   Abusive        13169 non-null  int64 \n",
      " 3   HS_Individual  13169 non-null  int64 \n",
      " 4   HS_Group       13169 non-null  int64 \n",
      " 5   HS_Religion    13169 non-null  int64 \n",
      " 6   HS_Race        13169 non-null  int64 \n",
      " 7   HS_Physical    13169 non-null  int64 \n",
      " 8   HS_Gender      13169 non-null  int64 \n",
      " 9   HS_Other       13169 non-null  int64 \n",
      " 10  HS_Weak        13169 non-null  int64 \n",
      " 11  HS_Moderate    13169 non-null  int64 \n",
      " 12  HS_Strong      13169 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values present: 48\n"
     ]
    }
   ],
   "source": [
    "count_nan = okkyibrahim['Tweet'].isnull().sum()\n",
    "print('Number of NaN values present: ' + str(count_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessed:**\n",
    "- Lower casing all text,\n",
    "- Data cleaning by removing unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation\n",
    "- Normalization using 'Alay' dictionary\n",
    "- Stemming using PySastrawi\n",
    "- Stop words removal using list\n",
    "\n",
    "Conclusion: *need to remove hexadecimal encoding, drop NaN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13121 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13121 non-null  int64 \n",
      " 2   Abusive        13121 non-null  int64 \n",
      " 3   HS_Individual  13121 non-null  int64 \n",
      " 4   HS_Group       13121 non-null  int64 \n",
      " 5   HS_Religion    13121 non-null  int64 \n",
      " 6   HS_Race        13121 non-null  int64 \n",
      " 7   HS_Physical    13121 non-null  int64 \n",
      " 8   HS_Gender      13121 non-null  int64 \n",
      " 9   HS_Other       13121 non-null  int64 \n",
      " 10  HS_Weak        13121 non-null  int64 \n",
      " 11  HS_Moderate    13121 non-null  int64 \n",
      " 12  HS_Strong      13121 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim = okkyibrahim.dropna(subset=['Tweet'])\n",
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deklarasi pilih kepala daerah 2018 aman anti h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gue selesai re watch aldnoah zero kampret 2 ka...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admin belanja po nak makan ais kepal milo ais ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enak ngewe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "6  deklarasi pilih kepala daerah 2018 aman anti h...   0        0   \n",
       "7  gue selesai re watch aldnoah zero kampret 2 ka...   0        1   \n",
       "8  admin belanja po nak makan ais kepal milo ais ...   0        0   \n",
       "9                                         enak ngewe   0        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "6              0         0            0        0            0          0   \n",
       "7              0         0            0        0            0          0   \n",
       "8              0         0            0        0            0          0   \n",
       "9              0         0            0        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "6         0        0            0          0  \n",
       "7         0        0            0          0  \n",
       "8         0        0            0          0  \n",
       "9         0        0            0          0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "okkyibrahim['Tweet'] = okkyibrahim['Tweet'].apply(clean_text)\n",
    "okkyibrahim.iloc[6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13121 entries, 0 to 13168\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          13121 non-null  object\n",
      " 1   HS             13121 non-null  int64 \n",
      " 2   Abusive        13121 non-null  int64 \n",
      " 3   HS_Individual  13121 non-null  int64 \n",
      " 4   HS_Group       13121 non-null  int64 \n",
      " 5   HS_Religion    13121 non-null  int64 \n",
      " 6   HS_Race        13121 non-null  int64 \n",
      " 7   HS_Physical    13121 non-null  int64 \n",
      " 8   HS_Gender      13121 non-null  int64 \n",
      " 9   HS_Other       13121 non-null  int64 \n",
      " 10  HS_Weak        13121 non-null  int64 \n",
      " 11  HS_Moderate    13121 non-null  int64 \n",
      " 12  HS_Strong      13121 non-null  int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "okkyibrahim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20894 entries, 0 to 20893\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            20894 non-null  object\n",
      " 1   Hate Speech     20894 non-null  object\n",
      " 2   Abusive Speech  20894 non-null  object\n",
      " 3   SARA            20894 non-null  object\n",
      " 4   Radicalism      20894 non-null  object\n",
      " 5   Defamation      20894 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 979.5+ KB\n"
     ]
    }
   ],
   "source": [
    "new_data = {\n",
    "    'Text': okkyibrahim['Tweet'],\n",
    "    'SARA': okkyibrahim[['HS_Religion', 'HS_Race']].max(axis=1),\n",
    "    'Radicalism': okkyibrahim['HS_Group'],\n",
    "    'Defamation': okkyibrahim['HS_Individual'],\n",
    "    'Hate Speech': okkyibrahim['HS'],\n",
    "    'Abusive Speech': okkyibrahim[['Abusive', 'HS_Gender', 'HS_Physical', 'HS_Other']].max(axis=1)\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "toxic_df = pd.concat([toxic_df, new_df], ignore_index=True)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from text_preprocess.text_preprocessing import preprocess_text\n",
    "\n",
    "toxic_df['Text'] = toxic_df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Hate Speech</th>\n",
       "      <th>Abusive Speech</th>\n",
       "      <th>SARA</th>\n",
       "      <th>Radicalism</th>\n",
       "      <th>Defamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jabar provinsi barokah nwoi anjing bodoh nprop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nitizen penasaran keluarga situ urus nya diuru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sidangahok moga sipenista agama ateknya mati w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakarta barusan baca undang beda presiden mant...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anak melulu nof nkaga mikir kasi anak malu lak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Hate Speech  \\\n",
       "0  jabar provinsi barokah nwoi anjing bodoh nprop...           1   \n",
       "1  nitizen penasaran keluarga situ urus nya diuru...           0   \n",
       "2  sidangahok moga sipenista agama ateknya mati w...           1   \n",
       "3  jakarta barusan baca undang beda presiden mant...           0   \n",
       "4  anak melulu nof nkaga mikir kasi anak malu lak...           0   \n",
       "\n",
       "  Abusive Speech SARA Radicalism Defamation  \n",
       "0              0    0          0          1  \n",
       "1              0    0          0          0  \n",
       "2              0    1          1          1  \n",
       "3              0    0          0          0  \n",
       "4              0    0          0          0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20894 entries, 0 to 20893\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            20894 non-null  object\n",
      " 1   Hate Speech     20894 non-null  int64 \n",
      " 2   Abusive Speech  20894 non-null  int64 \n",
      " 3   SARA            20894 non-null  int64 \n",
      " 4   Radicalism      20894 non-null  int64 \n",
      " 5   Defamation      20894 non-null  int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 979.5+ KB\n"
     ]
    }
   ],
   "source": [
    "category_columns = ['Hate Speech', 'Abusive Speech', 'SARA', 'Radicalism', 'Defamation']\n",
    "toxic_df[category_columns] = toxic_df[category_columns].astype(int)\n",
    "\n",
    "toxic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hate Speech       8701\n",
       "Abusive Speech    8336\n",
       "SARA              2502\n",
       "Radicalism        3261\n",
       "Defamation        5992\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df[category_columns].apply(lambda col: (col == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE with TF-IDF Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df['labels'] = toxic_df[category_columns].apply(lambda row: [cat for cat in category_columns if row[cat] == 1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF with limited features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(toxic_df['Text'])\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize multilabels\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(toxic_df['labels'])\n",
    "\n",
    "X_dense = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation loop for each label\n",
    "synthetic_records = []\n",
    "target_count = Y.sum(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8336,30811) (12558,30811) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m neg_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_bin \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# limit = min(5000, len(pos_indices))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# selected_pos = np.random.choice(pos_indices, limit, replace=False)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# selected_neg = np.random.choice(neg_indices, limit, replace=False)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# X_sub = X[selected_pos.tolist() + selected_neg.tolist()]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# y_sub = y_bin[selected_pos.tolist() + selected_neg.tolist()]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X_sub \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_bin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_bin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m y_sub \u001b[38;5;241m=\u001b[39m (y_bin[y_bin \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()) \u001b[38;5;241m+\u001b[39m (y_bin[y_bin \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     20\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m1\u001b[39m: target_count})\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8336,30811) (12558,30811) "
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(mlb.classes_):\n",
    "    y_bin = Y[:, i]\n",
    "    current_count = np.sum(y_bin)\n",
    "    \n",
    "    if current_count < 2:\n",
    "        print(f\"Skipping label {label} due to too few samples.\")\n",
    "        continue\n",
    "    \n",
    "    pos_indices = np.where(y_bin == 1)[0]\n",
    "    neg_indices = np.where(y_bin == 0)[0]\n",
    "    limit = min(5000, len(pos_indices))\n",
    "    selected_pos = np.random.choice(pos_indices, limit, replace=False)\n",
    "    selected_neg = np.random.choice(neg_indices, limit, replace=False)\n",
    "\n",
    "    X_sub = X[selected_pos.tolist() + selected_neg.tolist()]\n",
    "    y_sub = y_bin[selected_pos.tolist() + selected_neg.tolist()]\n",
    "\n",
    "    smote = SMOTE(sampling_strategy={1: target_count})\n",
    "    X_res, y_res = smote.fit_resample(X_sub.toarray(), y_sub)\n",
    "\n",
    "    n_synthetic = len(X_res) - len(X_sub.toarray())\n",
    "    X_synthetic = X_res[-n_synthetic:]\n",
    "\n",
    "    for vec in X_synthetic:\n",
    "        top_indices = vec.argsort()[-7:][::-1]\n",
    "        top_words = feature_names[top_indices]\n",
    "        sentence = \" \".join(top_words)\n",
    "        synthetic_records.append({'Text': sentence, label: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and clean synthetic data\n",
    "synthetic_df = pd.DataFrame(synthetic_records).fillna(0)\n",
    "for col in category_columns:\n",
    "    if col not in synthetic_df.columns:\n",
    "        synthetic_df[col] = 0\n",
    "synthetic_df = synthetic_df[[\"Text\"] + category_columns]\n",
    "synthetic_df[category_columns] = synthetic_df[category_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22742 entries, 0 to 22741\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            22742 non-null  object\n",
      " 1   Hate Speech     22742 non-null  int64 \n",
      " 2   Abusive Speech  22742 non-null  int64 \n",
      " 3   SARA            22742 non-null  int64 \n",
      " 4   Radicalism      22742 non-null  int64 \n",
      " 5   Defamation      22742 non-null  int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "synthetic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hate Speech       3701\n",
       "Abusive Speech    3701\n",
       "SARA              6199\n",
       "Radicalism        5440\n",
       "Defamation        3701\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df[category_columns].apply(lambda col: (col == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and synthetic\n",
    "final_df = pd.concat([toxic_df[[\"Text\"] + category_columns], synthetic_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43636 entries, 0 to 43635\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Text            43636 non-null  object\n",
      " 1   Hate Speech     43636 non-null  int64 \n",
      " 2   Abusive Speech  43636 non-null  int64 \n",
      " 3   SARA            43636 non-null  int64 \n",
      " 4   Radicalism      43636 non-null  int64 \n",
      " 5   Defamation      43636 non-null  int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hate Speech       12402\n",
       "Abusive Speech    12037\n",
       "SARA               8701\n",
       "Radicalism         8701\n",
       "Defamation         9693\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[category_columns].apply(lambda col: (col == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndoBERTweet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list = []\n",
    "test_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = toxic_df.drop(columns=category_columns)  # Features (input data)\n",
    "y = toxic_df[category_columns]  # Labels (multilabel binary matrix)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_values = X.values\n",
    "y_values = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iterative_train_test_split to split the data\n",
    "X_train, y_train, X_test, y_test = iterative_train_test_split(X_values, y_values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resulting numpy arrays back to DataFrames\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=y.columns)\n",
    "y_test_df = pd.DataFrame(y_test, columns=y.columns)\n",
    "\n",
    "# Combine the features and labels back into single dataframes\n",
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "test_df = pd.concat([X_test_df, y_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16715\n",
      "Testing set size: 4179\n",
      "\n",
      "Class distribution in training set:\n",
      "Hate Speech       4442\n",
      "Abusive Speech    4442\n",
      "SARA              5354\n",
      "Pornography       1398\n",
      "Radicalism        1021\n",
      "Defamation        4794\n",
      "dtype: int64\n",
      "\n",
      "Class distribution in testing set:\n",
      "Hate Speech       1110\n",
      "Abusive Speech    1110\n",
      "SARA              1436\n",
      "Pornography        349\n",
      "Radicalism         255\n",
      "Defamation        1198\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the results\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")\n",
    "\n",
    "# Optionally, print the class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train_df.sum())\n",
    "\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(y_test_df.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobertweet-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(df, max_len=128):\n",
    "    return tokenizer(\n",
    "        df['Text'].tolist(), \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_len, \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize_text(train_df)\n",
    "test_encodings = tokenize_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[category_columns].values\n",
    "test_labels = test_df[category_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6271574365901246,\n",
       " 1: 0.6271574365901246,\n",
       " 2: 0.5203274810110821,\n",
       " 3: 1.9927277062470194,\n",
       " 4: 2.728534116878877,\n",
       " 5: 0.5811083298567654}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = len(np.array(y_train_df))\n",
    "n_classes = len(np.array(y_train_df)[0])\n",
    "\n",
    "# Count each class frequency\n",
    "class_count = [0] * n_classes\n",
    "for classes in np.array(y_train_df):\n",
    "    for index in range(n_classes):\n",
    "        if classes[index] != 0:\n",
    "            class_count[index] += 1\n",
    "\n",
    "# Compute class weights using balanced method\n",
    "class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "class_labels = range(len(class_weights))\n",
    "dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('indolem/indobertweet-base-uncased', num_labels=len(category_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(model.device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Set the loss function in the model\n",
    "model.config.loss = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Radit1812\\Documents\\Kuliah\\Skripsi\\safe-anonymous-chat-bot-telegram\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.33751451847228137\n",
      "Epoch 2/3, Loss: 0.23913268325716686\n",
      "Epoch 3/3, Loss: 0.20593995799858605\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize and weights update\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.append(logits.sigmoid().cpu().numpy())\n",
    "        true_labels.append(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Hate Speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.89      0.91      3069\n",
      "         1.0       0.73      0.82      0.77      1110\n",
      "\n",
      "    accuracy                           0.87      4179\n",
      "   macro avg       0.83      0.85      0.84      4179\n",
      "weighted avg       0.88      0.87      0.87      4179\n",
      "\n",
      "Classification report for Abusive Speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.89      0.91      3069\n",
      "         1.0       0.73      0.81      0.77      1110\n",
      "\n",
      "    accuracy                           0.87      4179\n",
      "   macro avg       0.83      0.85      0.84      4179\n",
      "weighted avg       0.88      0.87      0.87      4179\n",
      "\n",
      "Classification report for SARA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87      2743\n",
      "         1.0       0.76      0.76      0.76      1436\n",
      "\n",
      "    accuracy                           0.84      4179\n",
      "   macro avg       0.82      0.82      0.82      4179\n",
      "weighted avg       0.84      0.84      0.84      4179\n",
      "\n",
      "Classification report for Pornography:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98      3830\n",
      "         1.0       0.89      0.77      0.82       349\n",
      "\n",
      "    accuracy                           0.97      4179\n",
      "   macro avg       0.93      0.88      0.90      4179\n",
      "weighted avg       0.97      0.97      0.97      4179\n",
      "\n",
      "Classification report for Radicalism:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      3924\n",
      "         1.0       0.85      0.74      0.79       255\n",
      "\n",
      "    accuracy                           0.98      4179\n",
      "   macro avg       0.92      0.86      0.89      4179\n",
      "weighted avg       0.98      0.98      0.98      4179\n",
      "\n",
      "Classification report for Defamation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.85      0.85      2981\n",
      "         1.0       0.63      0.66      0.65      1198\n",
      "\n",
      "    accuracy                           0.79      4179\n",
      "   macro avg       0.75      0.75      0.75      4179\n",
      "weighted avg       0.80      0.79      0.79      4179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "for i, col in enumerate(category_columns):\n",
    "    print(f\"Classification report for {col}:\")\n",
    "    print(classification_report(true_labels[:, i], binary_predictions[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categories(sentence, model, tokenizer, threshold=0.5):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "    probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "    return probabilities[0], predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate Speech: Probability = 0.9516, Prediction = Yes\n",
      "Abusive Speech: Probability = 0.9625, Prediction = Yes\n",
      "SARA: Probability = 0.9626, Prediction = Yes\n",
      "Pornography: Probability = 0.0108, Prediction = No\n",
      "Radicalism: Probability = 0.0115, Prediction = No\n",
      "Defamation: Probability = 0.8836, Prediction = Yes\n"
     ]
    }
   ],
   "source": [
    "sentence = \"jokowi goblok korupsi mulu ga sholat\"\n",
    "probabilities, predictions = predict_categories(sentence, model, tokenizer)\n",
    "\n",
    "for i, category in enumerate(category_columns):\n",
    "    print(f\"{category}: Probability = {probabilities[i]:.4f}, Prediction = {'Yes' if predictions[i] == 1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model-export\\\\tokenizer_config.json',\n",
       " './model-export\\\\special_tokens_map.json',\n",
       " './model-export\\\\vocab.txt',\n",
       " './model-export\\\\added_tokens.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./model-export\"\n",
    "torch.save(model, f\"{output_dir}/model.pth\")\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
